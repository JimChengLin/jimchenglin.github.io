<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> FAST 2019 DistCache · 阿吉的博客</title><meta name="description" content="FAST 2019 DistCache - Jim Lin"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="/atom.xml" title="阿吉的博客"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">FAST 2019 DistCache</h1><div class="post-info">Apr 15, 2019</div><div class="post-content"><p>本周我读的最佳论文是 FAST 2019 DistCache. 缓存系统就和人生一样, 有时简单的小小变动就会产生巨大的意想不到的影响. /(ㄒoㄒ)/~~</p>
<a id="more"></a>
<h2 id="DistCache-背景知识"><a href="#DistCache-背景知识" class="headerlink" title="DistCache 背景知识"></a>DistCache 背景知识</h2><ul>
<li><p>Balls into bins 问题</p>
</li>
<li><p>The Power of Two Random Choices</p>
</li>
<li><p>LB 与 cache</p>
</li>
</ul>
<p>Balls into bins 问题讲的是如果有 M 个小球随机等概率地扔进 N 个垃圾桶, 那么装球最多的那个垃圾桶大概率会有多少个球?</p>
<p>我第一反应就是这应该是正态分布吧. 毕竟看起来就是一连串互相独立的随机决定. 可能正态分布的 bound 还不够 tight. 出于精力的原因, 我并没有去看数学推导过程. 这里直接列出结果.</p>
<p>\[ O ( \log n / \log \log n ) \]</p>
<p>上面那个公式计算的是 load. 假设有 64 个 node, LB 随机分配流量, 那么 load 最大的 node, 负载将会是平均负载的 log 32 / log log 32 = 2.7 倍. 那为了保证 SLA, 每个节点都必须预留出 70 % 的资源空闲以应对单纯因为负载不均造成的 brust. 我已经忽略掉所有常数项了, 结果真是太糟了...</p>
<p>当然现实可能没这么悲观. 一些负载过高的节点可以暂时从 LB 里摘掉. 看起来解决了一个问题, 但又有了一个新问题就是现在谁来管理 LB 的状态? 据说阿里搞云梯一, 上千台的集群就经常受偏压困扰. 单点就打崩了.</p>
<p>解决方案出奇的简单(起码理论上是这样的), 这就引出了 The Power of Two Random Choices. LB 在分流的时候, 随机比较两台(d = 2)后端服务器的 load, 选最小.</p>
<p>\[ O ( \log \log n / \log d ) \]</p>
<p>log d 是常数, 最后得</p>
<p>\[ O ( \log \log n ) \]</p>
<p>log log 32 = 1.2, 那么只要 20% 的资源就够了(忽略常数项的情况下 :-D)? 震惊!</p>
<p>最后一个问题是 LB 与 cache 的互补关系. 2 choices 可以解决无状态的 load balance 问题, 但这对存储不起作用. 因为 LB 没有办法按 load 分流, 比如 key A 存在服务器上 s 上, s 跑满了, LB 就能把 query redirect 到别的机器上吗?</p>
<p>Small Cache, Big Effect 这篇 paper 很大程度上解决了这个问题. LB 与 cache layer 存在很强的互补关系. LB 讨厌 imbalance, 但 cache 喜欢. query 越 skew, cache 效率越高.</p>
<p>直觉上, 我们当然知道绝大绝大多数情况下 cache 越多, 系统性能越好(除非 network 是瓶颈), 那么在一个多 node 的系统, cache 至少要多少才有足够的信心说这个系统没有热点呢? paper 给出的答案吓死人了. 居然只要 O(n log n). n 是 node 数量!</p>
<p>WTF? 居然与 key 的数量无关? 那假如有一亿个 key, 4 个节点, 岂不是只要 cache 6 组 key 就够? 这太反直觉了.</p>
<p>但看了推导居然觉得还是很容易理解的. 作者构造了一组恶意 query 来模拟最差的情况, 然后做了些高中程度的数学消元就好了.</p>
<p>有几点前提是这个证明巧妙的地方, 通过定义绕过了很多很难处理的问题.</p>
<ul>
<li><p>Key 的分布必须均匀, 即使产生 skew 也只能是因为类似 Bins into balls 那种自然抖动产生的. 换言之, 如果一亿组 key, 然后九千万都在服务器 A, 起码 small cache 是救不了的... 也就是 hash function 必须得是 perfect hash.</p>
</li>
<li><p>Cache 必须未卜先知, 知道哪些 Key 是 hottest 的, 尽管实际生产上是绝对做不到的. Cache 还必须拥有无限 capacity, 多少 query 都能吸收, 这个倒是还好, cache 一般就是快得飞起.</p>
</li>
</ul>
<p>然后因为 key 大体分布均匀, hottest objects 必然在 cache 里, O(n log n) 实际上是限制了单个 key 的被 query 次数上限来抵消 Bins into balls 中描述的自然抖动. 所以最终可以做到无论 query 什么分布, key 有多少, O(n log n) 组 cache 就够了.</p>
<h2 id="DistCache"><a href="#DistCache" class="headerlink" title="DistCache"></a>DistCache</h2><p>千呼万唤始出来, 终于到了 DistCache. 其实这部分跟 small cache 那篇差不多, 不复杂, 但突出一个巧. DistCache 的作者团队(Xin Jin)在折腾一个刷分项目(玩笑, 纯属推测), 根据 small cache 的理论, 只要单点够强, 很小的 cache 就可以刷很高的分. 那 cache 足够小是不是可以直接放进专有硬件使用专有协议了? 于是他们就把 LB 和 Cache 一起打包进了 switch 里... 然后就当然刷了很高的分.</p>
<p>插个话, 这个体现了无序 KV 是一个很好的&quot;设计&quot;或者说品位. 上层开发者好理解, 下层开发者好刷分. 天然可拆分, 可并发. 与此相同的应该就是&quot;流&quot;这个概念了. 一个比较糟糕的例子就是 JS 的隐式 type cast. 上层开发者搞不懂, 下层开发者也不好去掉 if 来加速.</p>
<p>言归正传, 刷分是无止境的. 即使用了专有硬件, 专有协议, 把交换机, LB, Cache 合并了, 还是有单点. 正所谓, 单点就刷不了分. KV 是可拆分的, 那很自然的想法是, 那多来几个 cache server 就好了啊.</p>
<p>新问题来了, 怎么确保 cache server 是 balance 的呢? cache server 已经是刷分最快的了, 没设备给 cache server 做防止 imbalance 的 cache 了, 囧(有点绕).</p>
<p>作者给出的方案很巧妙也很简单, 就是双层 cache. 当前所有 cache server 的所有 KV 用一个新的 hash function 存在另一层 cache layer 里. 从数学和直觉上可以确保, 任意一层的 cache server 出现了热点, 那么这个热点的 KV 数据在另一个 cache layer 是分散的, 无热点的.</p>
<p>然后在哪一层 cache 找 key 的问题又可以转化为 The Power of Two Random Choices 问题 - 直接选负载最低的那一就好了. 好了, 让我们继续刷分吧!</p>
<h2 id="体会"><a href="#体会" class="headerlink" title="体会"></a>体会</h2><p>数据存储冷热分离, 大有搞头! cache 远比我想象中的高效. 热数据存储就相当于冷数据的 cache + LB 了, 冷数据打散进大规模 HDD 集群. 热存储集群保证冷存集群不会出现热点, 那么冷存集群就可以维持一个比较好的 throughput. 整体就会很快, 不是吗?</p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/04/07/Cuckoo-Hashing/" class="next">NEXT</a></div><div class="copyright"><p>© 2018 - 2019 <a href>Jim Lin</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script></body></html>